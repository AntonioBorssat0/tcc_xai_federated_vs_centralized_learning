{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bee72d4",
   "metadata": {},
   "source": [
    "# Avaliação Comparativa de Modelos\n",
    "\n",
    "Este notebook compara o desempenho de todos os 5 modelos treinados:\n",
    "\n",
    "1. **MLP Centralizado** - Baseline centralizado\n",
    "2. **XGBoost Centralizado** - Baseline centralizado\n",
    "3. **MLP Federado (FedAvg)** - Aprendizado federado com média de pesos\n",
    "4. **XGBoost Federado (Bagging)** - Agregação bootstrap\n",
    "5. **XGBoost Federado (Cyclic)** - Treinamento cliente-por-cliente\n",
    "\n",
    "**Objetivo:** Avaliar todos os modelos no **mesmo conjunto de teste** para comparação justa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ee8bbf",
   "metadata": {},
   "source": [
    "## 1. Importar Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88bbc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    matthews_corrcoef,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "# Configurações de visualização\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeea15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar paths necessários para imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Adicionar diretórios ao Python path\n",
    "base_path = Path.cwd().parent\n",
    "sys.path.insert(0, str(base_path / \"flwr-mlp\"))\n",
    "sys.path.insert(0, str(base_path / \"centralized_training\"))\n",
    "\n",
    "print(\"Paths configurados:\")\n",
    "print(f\"   - {base_path / 'flwr-mlp'}\")\n",
    "print(f\"   - {base_path / 'centralized_training'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d61f6c5",
   "metadata": {},
   "source": [
    "## 2. Funções de Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc222ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constantes\n",
    "WIND_DIRS = [\"N\", \"NNE\", \"NE\", \"ENE\", \"E\", \"ESE\", \"SE\", \"SSE\", \n",
    "             \"S\", \"SSW\", \"SW\", \"WSW\", \"W\", \"WNW\", \"NW\", \"NNW\"]\n",
    "\n",
    "def encode_wind_directions(df):\n",
    "    \"\"\"\n",
    "    Converte direções de vento categóricas para componentes sin/cos.\n",
    "    Preserva a natureza circular das direções de vento.\n",
    "    \"\"\"\n",
    "    wind_cols = [\"WindGustDir\", \"WindDir9am\", \"WindDir3pm\"]\n",
    "    \n",
    "    for col in wind_cols:\n",
    "        if col in df.columns:\n",
    "            # Criar mapeamento de direções para ângulos (em radianos)\n",
    "            dir_to_rad = {dir_: i * 2 * np.pi / 16 for i, dir_ in enumerate(WIND_DIRS)}\n",
    "            \n",
    "            # Adicionar tratamento para valores ausentes\n",
    "            dir_to_rad.update({np.nan: 0, \"NA\": 0, None: 0})\n",
    "            \n",
    "            # Converter para ângulos e depois para componentes sin/cos\n",
    "            angles = df[col].map(dir_to_rad)\n",
    "            df[f\"{col}_sin\"] = np.sin(angles)\n",
    "            df[f\"{col}_cos\"] = np.cos(angles)\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def prepare_weather_data(df):\n",
    "    \"\"\"\n",
    "    Preprocessar dados meteorológicos:\n",
    "    - Encode direções de vento (categóricas → sin/cos)\n",
    "    - Remove coluna Location\n",
    "    \"\"\"\n",
    "    df = encode_wind_directions(df)\n",
    "    \n",
    "    if 'Location' in df.columns:\n",
    "        df = df.drop(columns=['Location'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Funções de pré-processamento definidas!\")\n",
    "print(f\"   - encode_wind_directions: Converte {len([c for c in ['WindGustDir', 'WindDir9am', 'WindDir3pm']])} colunas de vento\")\n",
    "print(f\"   - prepare_weather_data: Pipeline completo de preprocessamento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce0c169",
   "metadata": {},
   "source": [
    "## 3. Carregar e Preprocessar Dados de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a32878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "base_path = Path.cwd().parent\n",
    "data_path = base_path / \"datasets\" / \"rain_australia\" / \"weatherAUS_cleaned.csv\"\n",
    "test_indices_path = base_path / \"datasets\" / \"test_indices.csv\"\n",
    "\n",
    "print(\"Carregando dados...\")\n",
    "\n",
    "# Carregar dataset completo\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"   Dataset: {len(df):,} amostras\")\n",
    "print(f\"   Colunas originais: {len(df.columns)}\")\n",
    "\n",
    "# Carregar índices de teste\n",
    "test_indices = pd.read_csv(test_indices_path)['index'].values\n",
    "print(f\"   Conjunto de teste: {len(test_indices):,} amostras\")\n",
    "\n",
    "# Filtrar dados de teste\n",
    "df_test = df.iloc[test_indices].copy()\n",
    "\n",
    "# Separar target ANTES do preprocessing\n",
    "y_test = df_test['RainTomorrow'].values.astype(np.float32)\n",
    "\n",
    "print(f\"\\nAplicando pré-processamento...\")\n",
    "\n",
    "# Aplicar preprocessing (encode vento + remover Location)\n",
    "df_test_processed = prepare_weather_data(df_test.copy())\n",
    "\n",
    "print(f\"   Encoding de direções de vento aplicado (categórico → sin/cos)\")\n",
    "print(f\"   Colunas após preprocessing: {len(df_test_processed.columns)}\")\n",
    "\n",
    "# Verificar que WindGustDir, WindDir9am, WindDir3pm foram convertidas\n",
    "wind_encoded_cols = [col for col in df_test_processed.columns if 'WindDir' in col or 'WindGust' in col]\n",
    "print(f\"   Colunas de vento codificadas: {wind_encoded_cols}\")\n",
    "\n",
    "# Remover target das features\n",
    "if 'RainTomorrow' in df_test_processed.columns:\n",
    "    df_test_processed = df_test_processed.drop(columns=['RainTomorrow'])\n",
    "\n",
    "# Features para MLP\n",
    "X_test_mlp = df_test_processed.values.astype(np.float32)\n",
    "\n",
    "# Features para XGBoost (DataFrame)\n",
    "X_test_xgb = df_test_processed.copy()\n",
    "\n",
    "print(f\"\\nConjunto de teste preparado:\")\n",
    "print(f\"   No Rain (0): {(y_test == 0).sum():,} ({(y_test == 0).sum() / len(y_test) * 100:.1f}%)\")\n",
    "print(f\"   Rain (1): {(y_test == 1).sum():,} ({(y_test == 1).sum() / len(y_test) * 100:.1f}%)\")\n",
    "print(f\"   Features (MLP): {X_test_mlp.shape}\")\n",
    "print(f\"   Features (XGBoost): {X_test_xgb.shape}\")\n",
    "print(f\"   Feature names: {list(df_test_processed.columns)[:5]} ... (total: {len(df_test_processed.columns)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e91b273",
   "metadata": {},
   "source": [
    "## 4. Definir Arquitetura MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad5fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherMLP(nn.Module):\n",
    "    \"\"\"Rede Neural para previsão de chuva.\"\"\"\n",
    "    def __init__(self, input_size=20, hidden1=96, hidden2=112, hidden3=32, \n",
    "                 dropout1=0.14, dropout2=0.38):\n",
    "        super(WeatherMLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden1)\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden2)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden2, hidden3)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden3)\n",
    "        \n",
    "        self.fc4 = nn.Linear(hidden3, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "print(\"Arquitetura MLP definida!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c570204",
   "metadata": {},
   "source": [
    "## 5. Carregar Todos os Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a99052",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Carregando modelos...\\n\")\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "models = {}\n",
    "model_metadata = {}\n",
    "\n",
    "# 1. MLP Centralizado\n",
    "try:\n",
    "    mlp_cent_path = base_path / \"centralized_training\" / \"models\" / \"mlp\" / \"centralized_model_best.joblib\"\n",
    "    mlp_cent_data = joblib.load(mlp_cent_path)\n",
    "    \n",
    "    if isinstance(mlp_cent_data, dict):\n",
    "        models['MLP Centralizado'] = mlp_cent_data['best_model']\n",
    "        models['MLP Centralizado'].eval()\n",
    "        model_metadata['MLP Centralizado'] = mlp_cent_data.get('metadata', {})\n",
    "        model_metadata['MLP Centralizado']['scaler'] = mlp_cent_data.get('scaler')\n",
    "        \n",
    "        print(\"[OK] MLP Centralizado carregado\")\n",
    "        print(f\"   Input size: {mlp_cent_data.get('metadata', {}).get('input_size', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"[ERRO] Formato inesperado: {type(mlp_cent_data)}\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERRO] Erro ao carregar MLP Centralizado: {e}\")\n",
    "    import traceback\n",
    "    print(f\"   Detalhes: {traceback.format_exc()}\")\n",
    "\n",
    "# 2. XGBoost Centralizado\n",
    "try:\n",
    "    xgb_cent_path = base_path / \"centralized_training\" / \"models\" / \"xgboost\" / \"centralized_model_best.joblib\"\n",
    "    xgb_cent_data = joblib.load(xgb_cent_path)\n",
    "    \n",
    "    if isinstance(xgb_cent_data, dict):\n",
    "        models['XGBoost Centralizado'] = xgb_cent_data['best_model']\n",
    "        model_metadata['XGBoost Centralizado'] = xgb_cent_data.get('metadata', {})\n",
    "    else:\n",
    "        models['XGBoost Centralizado'] = xgb_cent_data\n",
    "        model_metadata['XGBoost Centralizado'] = {}\n",
    "    \n",
    "    print(\"[OK] XGBoost Centralizado carregado\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERRO] Erro ao carregar XGBoost Centralizado: {e}\")\n",
    "\n",
    "# 3. MLP Federado\n",
    "try:\n",
    "    mlp_fed_path = base_path / \"flwr-mlp\" / \"models\" / \"global_model_final.joblib\"\n",
    "    mlp_fed_data = joblib.load(mlp_fed_path)\n",
    "    \n",
    "    if isinstance(mlp_fed_data, dict):\n",
    "        models['MLP Federado (FedAvg)'] = mlp_fed_data['best_model']\n",
    "        models['MLP Federado (FedAvg)'].eval()\n",
    "        model_metadata['MLP Federado (FedAvg)'] = mlp_fed_data.get('metadata', {})\n",
    "        \n",
    "        if 'scaler' in mlp_fed_data:\n",
    "            model_metadata['MLP Federado (FedAvg)']['scaler'] = mlp_fed_data['scaler']\n",
    "        else:\n",
    "            model_metadata['MLP Federado (FedAvg)']['scaler'] = None\n",
    "        \n",
    "        print(\"[OK] MLP Federado (FedAvg) carregado\")\n",
    "    else:\n",
    "        models['MLP Federado (FedAvg)'] = mlp_fed_data\n",
    "        models['MLP Federado (FedAvg)'].eval()\n",
    "        model_metadata['MLP Federado (FedAvg)'] = {}\n",
    "        model_metadata['MLP Federado (FedAvg)']['scaler'] = None\n",
    "        \n",
    "        print(\"[OK] MLP Federado (FedAvg) carregado\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERRO] Erro ao carregar MLP Federado: {e}\")\n",
    "    import traceback\n",
    "    print(f\"   Detalhes: {traceback.format_exc()}\")\n",
    "\n",
    "# 4. XGBoost Federado Bagging\n",
    "try:\n",
    "    xgb_bag_path = base_path / \"flwr-xgboost\" / \"models\" / \"global_model_bagging_final.joblib\"\n",
    "    xgb_bag_data = joblib.load(xgb_bag_path)\n",
    "    \n",
    "    if isinstance(xgb_bag_data, dict):\n",
    "        if 'best_model' in xgb_bag_data:\n",
    "            models['XGBoost Federado (Bagging)'] = xgb_bag_data['best_model']\n",
    "            model_metadata['XGBoost Federado (Bagging)'] = xgb_bag_data.get('metadata', {})\n",
    "        else:\n",
    "            print(f\"   [INFO] Chaves disponíveis: {list(xgb_bag_data.keys())}\")\n",
    "            models['XGBoost Federado (Bagging)'] = xgb_bag_data\n",
    "            model_metadata['XGBoost Federado (Bagging)'] = {}\n",
    "    else:\n",
    "        models['XGBoost Federado (Bagging)'] = xgb_bag_data\n",
    "        model_metadata['XGBoost Federado (Bagging)'] = {}\n",
    "    \n",
    "    print(\"[OK] XGBoost Federado (Bagging) carregado\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERRO] Erro ao carregar XGBoost Federado Bagging: {e}\")\n",
    "\n",
    "# 5. XGBoost Federado Cyclic\n",
    "try:\n",
    "    xgb_cyc_path = base_path / \"flwr-xgboost\" / \"models\" / \"global_model_cyclic_final.joblib\"\n",
    "    xgb_cyc_data = joblib.load(xgb_cyc_path)\n",
    "    \n",
    "    if isinstance(xgb_cyc_data, dict):\n",
    "        if 'best_model' in xgb_cyc_data:\n",
    "            models['XGBoost Federado (Cyclic)'] = xgb_cyc_data['best_model']\n",
    "            model_metadata['XGBoost Federado (Cyclic)'] = xgb_cyc_data.get('metadata', {})\n",
    "        else:\n",
    "            print(f\"   [INFO] Chaves disponíveis: {list(xgb_cyc_data.keys())}\")\n",
    "            models['XGBoost Federado (Cyclic)'] = xgb_cyc_data\n",
    "            model_metadata['XGBoost Federado (Cyclic)'] = {}\n",
    "    else:\n",
    "        models['XGBoost Federado (Cyclic)'] = xgb_cyc_data\n",
    "        model_metadata['XGBoost Federado (Cyclic)'] = {}\n",
    "    \n",
    "    print(\"[OK] XGBoost Federado (Cyclic) carregado\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERRO] Erro ao carregar XGBoost Federado Cyclic: {e}\")\n",
    "\n",
    "print(f\"\\nTotal de modelos carregados: {len(models)}/5\")\n",
    "\n",
    "# Mostrar informações dos modelos\n",
    "print(f\"\\nInformações dos modelos:\")\n",
    "for model_name, metadata in model_metadata.items():\n",
    "    if metadata:\n",
    "        print(f\"\\n   {model_name}:\")\n",
    "        print(f\"      - AUC: {metadata.get('auc', 'N/A'):.4f}\" if isinstance(metadata.get('auc'), (int, float)) else f\"      - AUC: {metadata.get('auc', 'N/A')}\")\n",
    "        print(f\"      - Accuracy: {metadata.get('accuracy', 'N/A'):.4f}\" if isinstance(metadata.get('accuracy'), (int, float)) else f\"      - Accuracy: {metadata.get('accuracy', 'N/A')}\")\n",
    "        print(f\"      - Framework: {metadata.get('framework', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5bbcbd",
   "metadata": {},
   "source": [
    "## 5.5. Verificação de Integridade dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27edaa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verificando integridade dos modelos...\\n\")\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "verification_results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    result = {'Modelo': model_name}\n",
    "    \n",
    "    if 'MLP' in model_name:\n",
    "        # Verificar MLP\n",
    "        result['Tipo'] = 'PyTorch MLP'\n",
    "        result['Classe'] = type(model).__name__\n",
    "        result['Parâmetros'] = sum(p.numel() for p in model.parameters())\n",
    "        result['Input Size'] = model.fc1.in_features if hasattr(model, 'fc1') else 'N/A'\n",
    "        result['Status'] = '[OK]'\n",
    "    else:\n",
    "        # Verificar XGBoost\n",
    "        result['Tipo'] = 'XGBoost Booster'\n",
    "        result['Classe'] = type(model).__name__\n",
    "        \n",
    "        if isinstance(model, xgb.Booster):\n",
    "            result['Num Boosted Rounds'] = model.num_boosted_rounds()\n",
    "            result['Num Features'] = model.num_features()\n",
    "            result['Status'] = '[OK]'\n",
    "        elif isinstance(model, dict):\n",
    "            result['Status'] = '[ERRO] Ainda é dicionário'\n",
    "            result['Chaves'] = list(model.keys())\n",
    "        else:\n",
    "            result['Status'] = '[AVISO] Tipo inesperado'\n",
    "    \n",
    "    verification_results.append(result)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"=\"*80)\n",
    "print(\"VERIFICAÇÃO DE INTEGRIDADE DOS MODELOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for result in verification_results:\n",
    "    print(f\"\\n{result['Modelo']}\")\n",
    "    print(f\"   - Tipo: {result['Tipo']}\")\n",
    "    print(f\"   - Classe: {result['Classe']}\")\n",
    "    \n",
    "    if 'Input Size' in result:\n",
    "        print(f\"   - Input Size: {result['Input Size']}\")\n",
    "        print(f\"   - Parâmetros: {result['Parâmetros']:,}\")\n",
    "    elif 'Num Features' in result:\n",
    "        print(f\"   - Num Features: {result['Num Features']}\")\n",
    "        print(f\"   - Num Boosted Rounds: {result['Num Boosted Rounds']}\")\n",
    "    \n",
    "    print(f\"   - Status: {result['Status']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Verificar se há algum problema\n",
    "problemas = [r for r in verification_results if '[ERRO]' in r['Status'] or '[AVISO]' in r['Status']]\n",
    "if problemas:\n",
    "    print(\"\\n[ATENÇÃO] Foram encontrados problemas com os seguintes modelos:\")\n",
    "    for p in problemas:\n",
    "        print(f\"   - {p['Modelo']}: {p['Status']}\")\n",
    "else:\n",
    "    print(\"\\nTodos os modelos passaram na verificação de integridade!\")\n",
    "    print(\"   Modelos prontos para gerar predições\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef26c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CÉLULA DE DEBUG ATUALIZADA\n",
    "\n",
    "print(\"DIAGNÓSTICO COMPLETO DO PROBLEMA MLP\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Carregar modelo\n",
    "mlp_cent_data = joblib.load(base_path / \"centralized_training\" / \"models\" / \"mlp\" / \"centralized_model_best.joblib\")\n",
    "\n",
    "# 2. Verificar scaler\n",
    "scaler = mlp_cent_data['scaler']\n",
    "print(\"[OK] Scaler encontrado\")\n",
    "print(f\"   N features esperadas: {scaler.n_features_in_}\")\n",
    "\n",
    "# 3. Inspecionar arquitetura do modelo salvo\n",
    "model = mlp_cent_data['best_model']\n",
    "print(f\"\\nArquitetura do modelo SALVO:\")\n",
    "print(f\"   Tipo: {type(model)}\")\n",
    "print(f\"   Módulos:\")\n",
    "\n",
    "for name, module in model.named_children():\n",
    "    print(f\"      - {name}: {module}\")\n",
    "    if hasattr(module, 'in_features'):\n",
    "        print(f\"         in_features: {module.in_features}\")\n",
    "    if hasattr(module, 'out_features'):\n",
    "        print(f\"         out_features: {module.out_features}\")\n",
    "\n",
    "# 4. Tentar acessar primeira camada linear\n",
    "try:\n",
    "    first_linear = None\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            first_linear = module\n",
    "            print(f\"\\nPrimeira camada Linear encontrada: '{name}'\")\n",
    "            print(f\"   Input size: {first_linear.in_features}\")\n",
    "            break\n",
    "    \n",
    "    if first_linear:\n",
    "        expected_features = first_linear.in_features\n",
    "        print(f\"\\n[OK] Features esperadas pelo modelo: {expected_features}\")\n",
    "        print(f\"Features no conjunto de teste: {X_test_mlp.shape[1]}\")\n",
    "        \n",
    "        if expected_features != X_test_mlp.shape[1]:\n",
    "            print(f\"\\n[ERRO] INCOMPATIBILIDADE!\")\n",
    "            print(f\"   Modelo espera: {expected_features}\")\n",
    "            print(f\"   Teste tem: {X_test_mlp.shape[1]}\")\n",
    "            print(f\"   SOLUÇÃO: Usar scaler.n_features_in_ = {scaler.n_features_in_}\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERRO] Erro ao inspecionar: {e}\")\n",
    "\n",
    "# 5. Verificar feature names\n",
    "if 'feature_names' in mlp_cent_data:\n",
    "    train_features = mlp_cent_data['feature_names']\n",
    "    test_features = list(df_test_processed.columns)\n",
    "    \n",
    "    print(f\"\\nFeatures no TREINAMENTO ({len(train_features)}):\")\n",
    "    print(f\"   {train_features[:5]} ... (total: {len(train_features)})\")\n",
    "    \n",
    "    print(f\"\\nFeatures no TESTE ({len(test_features)}):\")\n",
    "    print(f\"   {test_features[:5]} ... (total: {len(test_features)})\")\n",
    "    \n",
    "    if len(train_features) != len(test_features):\n",
    "        print(f\"\\n[ERRO] QUANTIDADE DIFERENTE!\")\n",
    "        print(f\"   Treino: {len(train_features)} features\")\n",
    "        print(f\"   Teste: {len(test_features)} features\")\n",
    "        \n",
    "        missing = set(train_features) - set(test_features)\n",
    "        if missing:\n",
    "            print(f\"\\n[INFO] Features no treino MAS NÃO no teste:\")\n",
    "            print(f\"   {sorted(missing)}\")\n",
    "        \n",
    "        extra = set(test_features) - set(train_features)\n",
    "        if extra:\n",
    "            print(f\"\\n[INFO] Features no teste MAS NÃO no treino:\")\n",
    "            print(f\"   {sorted(extra)}\")\n",
    "else:\n",
    "    print(\"\\n[INFO] Feature names não salvos\")\n",
    "\n",
    "# 6. Teste com dados ESCALADOS\n",
    "print(f\"\\nTeste com 1 amostra (SEM e COM scaler):\")\n",
    "\n",
    "# SEM scaler\n",
    "X_sample_raw = X_test_mlp[:10].copy()\n",
    "print(f\"\\n   RAW (sem scaler):\")\n",
    "print(f\"      Shape: {X_sample_raw.shape}\")\n",
    "print(f\"      Min: {X_sample_raw.min():.4f}, Max: {X_sample_raw.max():.4f}, Mean: {X_sample_raw.mean():.4f}\")\n",
    "\n",
    "# COM scaler\n",
    "X_sample_scaled = scaler.transform(X_sample_raw)\n",
    "print(f\"\\n   SCALED (com scaler):\")\n",
    "print(f\"      Shape: {X_sample_scaled.shape}\")\n",
    "print(f\"      Min: {X_sample_scaled.min():.4f}, Max: {X_sample_scaled.max():.4f}, Mean: {X_sample_scaled.mean():.4f}\")\n",
    "\n",
    "# Predições\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output_raw = model(torch.FloatTensor(X_sample_raw[:1]))\n",
    "    prob_raw = torch.sigmoid(output_raw).item()\n",
    "    \n",
    "    output_scaled = model(torch.FloatTensor(X_sample_scaled[:1]))\n",
    "    prob_scaled = torch.sigmoid(output_scaled).item()\n",
    "    \n",
    "    print(f\"\\n   Predições:\")\n",
    "    print(f\"      SEM scaler: prob={prob_raw:.4f}, pred={1 if prob_raw >= 0.5 else 0}\")\n",
    "    print(f\"      COM scaler: prob={prob_scaled:.4f}, pred={1 if prob_scaled >= 0.5 else 0}\")\n",
    "    print(f\"      Diferença: {abs(prob_raw - prob_scaled):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b83a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CÉLULA: Teste de Validação Pós-Correção (VERSÃO CORRIGIDA)\n",
    "print(\"Teste de validação pós-correção:\\n\")\n",
    "\n",
    "for model_name in models.keys():\n",
    "    if 'MLP' in model_name:\n",
    "        scaler = model_metadata[model_name].get('scaler')\n",
    "        model = models[model_name]\n",
    "        \n",
    "        if scaler is None:\n",
    "            print(f\"[ERRO] {model_name}: Scaler NÃO encontrado\")\n",
    "            print(f\"   Verificando arquivo original...\\n\")\n",
    "            \n",
    "            if 'Centralizado' in model_name:\n",
    "                path = base_path / \"centralized_training\" / \"models\" / \"mlp\" / \"centralized_model_best.joblib\"\n",
    "            else:\n",
    "                path = base_path / \"flwr-mlp\" / \"models\" / \"global_model_final.joblib\"\n",
    "            \n",
    "            data = joblib.load(path)\n",
    "            print(f\"   Chaves disponíveis: {list(data.keys())}\")\n",
    "            \n",
    "            if 'scaler' in data:\n",
    "                print(f\"   [OK] Scaler EXISTE no arquivo!\")\n",
    "                model_metadata[model_name]['scaler'] = data['scaler']\n",
    "                scaler = data['scaler']\n",
    "                \n",
    "                if scaler is not None and hasattr(scaler, 'n_features_in_'):\n",
    "                    print(f\"   Scaler configurado: {scaler.n_features_in_} features\\n\")\n",
    "                else:\n",
    "                    print(f\"   [ERRO] Scaler inválido! Usando fallback do modelo centralizado.\\n\")\n",
    "                    cent_data = joblib.load(base_path / \"centralized_training\" / \"models\" / \"mlp\" / \"centralized_model_best.joblib\")\n",
    "                    model_metadata[model_name]['scaler'] = cent_data['scaler']\n",
    "                    scaler = cent_data['scaler']\n",
    "                    print(f\"   Scaler fallback configurado: {scaler.n_features_in_} features\\n\")\n",
    "            else:\n",
    "                print(f\"   [INFO] Arquivo NÃO contém 'scaler'. Usando fallback do modelo centralizado.\")\n",
    "                cent_data = joblib.load(base_path / \"centralized_training\" / \"models\" / \"mlp\" / \"centralized_model_best.joblib\")\n",
    "                model_metadata[model_name]['scaler'] = cent_data['scaler']\n",
    "                scaler = cent_data['scaler']\n",
    "                print(f\"   Scaler fallback configurado: {scaler.n_features_in_} features\\n\")\n",
    "        \n",
    "        if scaler is None or not hasattr(scaler, 'transform'):\n",
    "            print(f\"[ERRO] {model_name}: Scaler ainda é None após recarregamento!\")\n",
    "            continue\n",
    "        \n",
    "        # Testar com 1 amostra\n",
    "        X_sample = X_test_mlp[:1]\n",
    "        X_scaled = scaler.transform(X_sample)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(torch.FloatTensor(X_scaled))\n",
    "            prob = torch.sigmoid(output).item()\n",
    "        \n",
    "        print(f\"[OK] {model_name}:\")\n",
    "        print(f\"   - Scaler: {scaler.n_features_in_} features\")\n",
    "        print(f\"   - Prob: {prob:.4f}\")\n",
    "        print(f\"   - Status: {'[OK]' if 0.0 < prob < 1.0 else '[PROBLEMA]'}\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Validação completa! Todos os modelos MLP têm scalers configurados.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3ce1a8",
   "metadata": {},
   "source": [
    "## 6. Gerar Predições para Todos os Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4793cc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CÉLULA CORRIGIDA: Gerar Predições\n",
    "\n",
    "print(\"Gerando predições...\\n\")\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando device: {device}\\n\")\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    try:\n",
    "        if 'MLP' in model_name:\n",
    "            \n",
    "            if 'Centralizado' in model_name:\n",
    "                scaler = model_metadata[model_name].get('scaler')\n",
    "                \n",
    "                if scaler is None:\n",
    "                    print(f\"   [ERRO] {model_name}: Scaler não encontrado no checkpoint!\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"   [OK] {model_name}: Usando scaler do TREINAMENTO (salvo no checkpoint)\")\n",
    "                print(f\"      Fitted com {scaler.n_features_in_} features\")\n",
    "                \n",
    "            else:  # Federado\n",
    "                print(f\"   [OK] {model_name}: Criando NOVO scaler (comportamento federado)\")\n",
    "                print(f\"      Cada cliente treinou com seu próprio scaler local\")\n",
    "                print(f\"      SHAP também usa novo scaler fitted nos dados de teste\")\n",
    "                \n",
    "                scaler = StandardScaler()\n",
    "                scaler.fit(X_test_mlp)\n",
    "                print(f\"      Scaler fitted com {X_test_mlp.shape[1]} features do teste\")\n",
    "            \n",
    "            X_test_scaled = scaler.transform(X_test_mlp)\n",
    "            \n",
    "            model = model.to(device)\n",
    "            X_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(X_tensor).squeeze()\n",
    "                probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "                preds = (probs >= 0.5).astype(int)\n",
    "            \n",
    "            predictions[model_name] = preds\n",
    "            probabilities[model_name] = probs\n",
    "            \n",
    "        else:\n",
    "            if isinstance(model, dict):\n",
    "                if 'best_model' in model:\n",
    "                    xgb_model = model['best_model']\n",
    "                elif 'booster' in model:\n",
    "                    xgb_model = model['booster']\n",
    "                elif 'model' in model:\n",
    "                    xgb_model = model['model']\n",
    "                else:\n",
    "                    print(f\"   [INFO] {model_name}: Chaves disponíveis: {list(model.keys())}\")\n",
    "                    raise ValueError(f\"Não foi possível encontrar o modelo no dicionário\")\n",
    "            else:\n",
    "                xgb_model = model\n",
    "            \n",
    "            dtest = xgb.DMatrix(X_test_xgb)\n",
    "            probs = xgb_model.predict(dtest)\n",
    "            \n",
    "            if probs.max() > 1.0 or probs.min() < 0.0:\n",
    "                probs = 1 / (1 + np.exp(-probs))\n",
    "            \n",
    "            preds = (probs >= 0.5).astype(int)\n",
    "            \n",
    "            predictions[model_name] = preds\n",
    "            probabilities[model_name] = probs\n",
    "        \n",
    "        print(f\"   [OK] {model_name}: {len(preds):,} predições\")\n",
    "        print(f\"      Prob range: [{probs.min():.4f}, {probs.max():.4f}]\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   [ERRO] Erro ao fazer predições com {model_name}: {e}\")\n",
    "        import traceback\n",
    "        print(f\"      Detalhes: {traceback.format_exc()}\")\n",
    "\n",
    "print(f\"\\nPredições geradas para {len(predictions)} modelos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5e4aef",
   "metadata": {},
   "source": [
    "## Calcular Métricas para Todos os Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccead663",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculando métricas...\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name in predictions.keys():\n",
    "    y_pred = predictions[model_name]\n",
    "    y_prob = probabilities[model_name]\n",
    "    \n",
    "    # Calcular métricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    aucpr = average_precision_score(y_test, y_prob)\n",
    "    \n",
    "    # Matriz de confusão\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    results.append({\n",
    "        'Modelo': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'AUCPR': aucpr,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'MCC': mcc,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn,\n",
    "        'TP': tp\n",
    "    })\n",
    "\n",
    "# Criar DataFrame com resultados\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Ordenar por AUCPR (descendente)\n",
    "results_df = results_df.sort_values('AUCPR', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Métricas calculadas!\\n\")\n",
    "print(\"=\" * 120)\n",
    "print(\"RESULTADOS COMPARATIVOS\")\n",
    "print(\"=\" * 120)\n",
    "print(results_df[['Modelo', 'AUCPR', 'Precision', 'Recall', 'F1-Score', 'MCC']].to_string(index=False))\n",
    "print(\"=\" * 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1da3a0f",
   "metadata": {},
   "source": [
    "## 7. Classification Reports Detalhados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971ea911",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"CLASSIFICATION REPORTS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for model_name in predictions.keys():\n",
    "    print(f\"\\n{'-' * 100}\")\n",
    "    print(f\"{model_name}\")\n",
    "    print(f\"{'-' * 100}\")\n",
    "    \n",
    "    y_pred = predictions[model_name]\n",
    "    \n",
    "    # Classification report\n",
    "    report = classification_report(y_test, y_pred, \n",
    "                                   target_names=['No Rain', 'Rain'],\n",
    "                                   digits=4)\n",
    "    print(report)\n",
    "    \n",
    "    # Resumo adicional\n",
    "    aucpr = results_df[results_df['Modelo'] == model_name]['AUCPR'].values[0]\n",
    "    roc_auc = results_df[results_df['Modelo'] == model_name]['ROC-AUC'].values[0]\n",
    "    mcc = results_df[results_df['Modelo'] == model_name]['MCC'].values[0]\n",
    "    \n",
    "    print(f\"AUCPR (Average Precision): {aucpr:.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
    "\n",
    "print(f\"\\n{'=' * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d601e18f",
   "metadata": {},
   "source": [
    "## 8. Visualizar Matrizes de Confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea571c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gerando matrizes de confusão...\\n\")\n",
    "\n",
    "n_models = len(predictions)\n",
    "\n",
    "# FIGURA 1: MATRIZES NORMALIZADAS POR LINHA (RECALL)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(9, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, model_name in enumerate(predictions.keys()):\n",
    "    y_pred = predictions[model_name]\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    cm_normalized_row = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    annot_labels = np.empty_like(cm, dtype=object)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            annot_labels[i, j] = f'{cm[i, j]}\\n({cm_normalized_row[i, j]*100:.1f}%)'\n",
    "    \n",
    "    sns.heatmap(cm_normalized_row, annot=annot_labels, fmt='', cmap='Blues', \n",
    "                xticklabels=['No Rain', 'Rain'],\n",
    "                yticklabels=['No Rain', 'Rain'],\n",
    "                ax=axes[idx],\n",
    "                cbar_kws={'label': 'Proportion'},\n",
    "                annot_kws={'fontsize': 10, 'fontweight': 'bold'},\n",
    "                vmin=0, vmax=1)\n",
    "    \n",
    "    aucpr = results_df[results_df['Modelo'] == model_name]['AUCPR'].values[0]\n",
    "    f1 = results_df[results_df['Modelo'] == model_name]['F1-Score'].values[0]\n",
    "\n",
    "    axes[idx].set_title(f'{model_name}\\nAUCPR: {aucpr:.4f}\\nF1-Score: {f1:.4f}', \n",
    "                        fontsize=13, fontweight='bold', pad=10)\n",
    "    axes[idx].set_xlabel('Predicted', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Actual', fontsize=11, fontweight='bold')\n",
    "\n",
    "if n_models < 6:\n",
    "    fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(base_path / 'notebooks' / 'confusion_matrices_normalized_row.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Gráfico 1 salvo: confusion_matrices_normalized_row.png\")\n",
    "plt.show()\n",
    "\n",
    "# FIGURA 2: MATRIZES NORMALIZADAS POR COLUNA (PRECISION)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(9, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, model_name in enumerate(predictions.keys()):\n",
    "    y_pred = predictions[model_name]\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    cm_normalized_col = cm.astype('float') / cm.sum(axis=0)[np.newaxis, :]\n",
    "    \n",
    "    annot_labels = np.empty_like(cm, dtype=object)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            annot_labels[i, j] = f'{cm[i, j]}\\n({cm_normalized_col[i, j]*100:.1f}%)'\n",
    "    \n",
    "    sns.heatmap(cm_normalized_col, annot=annot_labels, fmt='', cmap='Greens', \n",
    "                xticklabels=['No Rain', 'Rain'],\n",
    "                yticklabels=['No Rain', 'Rain'],\n",
    "                ax=axes[idx],\n",
    "                cbar_kws={'label': 'Proportion'},\n",
    "                annot_kws={'fontsize': 10, 'fontweight': 'bold'},\n",
    "                vmin=0, vmax=1)\n",
    "    \n",
    "    aucpr = results_df[results_df['Modelo'] == model_name]['AUCPR'].values[0]\n",
    "    f1 = results_df[results_df['Modelo'] == model_name]['F1-Score'].values[0]\n",
    "    \n",
    "    axes[idx].set_title(f'{model_name}\\nAUCPR: {aucpr:.4f}\\nF1-Score: {f1:.4f}', \n",
    "                        fontsize=13, fontweight='bold', pad=10)\n",
    "    axes[idx].set_xlabel('Predicted', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Actual', fontsize=11, fontweight='bold')\n",
    "\n",
    "if n_models < 6:\n",
    "    fig.delaxes(axes[5])\n",
    "\n",
    "plt.suptitle('Matrizes de Confusão - Normalizado por Coluna (Precision)', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(base_path / 'notebooks' / 'confusion_matrices_normalized_col.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Gráfico 2 salvo: confusion_matrices_normalized_col.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Todas as matrizes de confusão foram geradas!\")\n",
    "print(\"   2 figuras salvas:\")\n",
    "print(\"      1. confusion_matrices_normalized_row.png (normalizado por linha - Blues)\")\n",
    "print(\"      2. confusion_matrices_normalized_col.png (normalizado por coluna - Greens)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef29a6f",
   "metadata": {},
   "source": [
    "## 9. Comparação Visual de Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8942a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados para visualização\n",
    "metrics_to_plot = ['AUCPR', 'Precision', 'Recall', 'F1-Score', 'MCC', 'ROC-AUC']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    plot_data = results_df.sort_values(metric, ascending=True)\n",
    "    \n",
    "    bars = axes[idx].barh(plot_data['Modelo'], plot_data[metric])\n",
    "    \n",
    "    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(plot_data)))\n",
    "    for bar, color in zip(bars, colors):\n",
    "        bar.set_color(color)\n",
    "    \n",
    "    for i, (bar, value) in enumerate(zip(bars, plot_data[metric])):\n",
    "        axes[idx].text(value + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                      f'{value:.4f}', \n",
    "                      va='center', fontsize=9)\n",
    "    \n",
    "    axes[idx].set_xlabel(metric, fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlim(0, 1.1)\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "    axes[idx].set_title(f'Comparação: {metric}', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(base_path / 'notebooks' / 'metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Gráfico salvo: metrics_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce90788",
   "metadata": {},
   "source": [
    "## 10. Análise de Trade-offs: Precision vs Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dd1543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Precision vs Recall\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "colors = plt.cm.Set2(np.arange(len(results_df)))\n",
    "\n",
    "for idx, row in results_df.iterrows():\n",
    "    plt.scatter(row['Recall'], row['Precision'], \n",
    "               s=500, alpha=0.6, color=colors[idx],\n",
    "               edgecolors='black', linewidth=2)\n",
    "    \n",
    "    plt.annotate(row['Modelo'], \n",
    "                xy=(row['Recall'], row['Precision']),\n",
    "                xytext=(10, 5), textcoords='offset points',\n",
    "                fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.xlabel('Recall (Sensibilidade)', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('Precision', fontsize=13, fontweight='bold')\n",
    "plt.title('Trade-off: Precision vs Recall\\n(Maior = Melhor para ambos)', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0.45, 0.85)\n",
    "plt.ylim(0.45, 0.85)\n",
    "\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.3, label='Baseline (0.5)')\n",
    "plt.axvline(x=0.5, color='red', linestyle='--', alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(base_path / 'notebooks' / 'precision_recall_tradeoff.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Gráfico salvo: precision_recall_tradeoff.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c100985",
   "metadata": {},
   "source": [
    "## 11. Ranking Final dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b342f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"RANKING FINAL DOS MODELOS (por AUCPR)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for idx, row in results_df.iterrows():\n",
    "    position = ['1º', '2º', '3º', '4º', '5º'][idx]\n",
    "    print(f\"\\n{position} Lugar: {row['Modelo']}\")\n",
    "    print(f\"   - AUCPR: {row['AUCPR']:.4f}\")\n",
    "    print(f\"   - ROC-AUC: {row['ROC-AUC']:.4f}\")\n",
    "    print(f\"   - Precision: {row['Precision']:.4f}\")\n",
    "    print(f\"   - Recall: {row['Recall']:.4f}\")\n",
    "    print(f\"   - F1-Score: {row['F1-Score']:.4f}\")\n",
    "    print(f\"   - MCC: {row['MCC']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a9181b",
   "metadata": {},
   "source": [
    "## 12. Salvar Resultados em CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5adeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar resultados completos\n",
    "output_path = base_path / 'notebooks' / 'model_comparison_results.csv'\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"Resultados salvos em: {output_path}\")\n",
    "\n",
    "# Salvar também uma versão formatada\n",
    "output_path_formatted = base_path / 'notebooks' / 'model_comparison_formatted.txt'\n",
    "with open(output_path_formatted, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"=\" * 100 + \"\\n\")\n",
    "    f.write(\"COMPARAÇÃO DE MODELOS - RESULTADOS FINAIS\\n\")\n",
    "    f.write(\"=\" * 100 + \"\\n\\n\")\n",
    "    f.write(results_df.to_string(index=False))\n",
    "    f.write(\"\\n\\n\" + \"=\" * 100 + \"\\n\")\n",
    "\n",
    "print(f\"Resultados formatados salvos em: {output_path_formatted}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcc-pytorch-ZStE8zF2-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
