# =====================================================================
# Federated XGBoost for Australian Weather Rain Prediction
# Supports both Bagging and Cyclic training strategies
# =====================================================================

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "flwr-xgboost"
version = "1.0.0"
description = "Federated XGBoost for Australian Weather Rain Prediction (Bagging + Cyclic)"
license = "Apache-2.0"
dependencies = [
    "flwr[simulation]>=1.22.0",
    "flwr-datasets>=0.5.0",
    "xgboost>=2.0.0",
    "pandas>=2.0.0",
    "numpy>=1.24.0",
    "scikit-learn>=1.3.0",
]

[tool.hatch.build.targets.wheel]
packages = ["."]

[tool.flwr.app]
publisher = "Antonio"

[tool.flwr.app.components]
serverapp = "federated_xgboost.server_app:app"
clientapp = "federated_xgboost.client_app:app"

# ===== TRAINING STRATEGY =====
# Choose: "bagging" or "cyclic"
[tool.flwr.app.config]
strategy = "cyclic"  # Options: "bagging" or "cyclic"

# ===== CYCLIC STRATEGY CONFIGURATION =====
# Sequential training: one client per round, each adds trees incrementally
# Rationale: 44 clients × 5 trees = 220 trees
num-server-rounds-cyclic = 44    # 1 round per client (44 locations, fixed by architecture)
local-epochs-cyclic = 5          # 5 trees per client (total: 44×5 = 220 trees)

# ===== BAGGING STRATEGY CONFIGURATION =====
# Parallel training: all clients train simultaneously, then aggregate
# Rationale: Fixed data partitions → Only 1 round to avoid overfitting
# With fixed partitions, multiple rounds = re-training on same data = memorization!
# Pure bagging: 1 round × 44 clients × 5 trees = 220 trees (aligned with cyclic)
num-server-rounds-bagging = 1    # Only 1 round (pure bagging, no overfitting)
local-epochs-bagging = 5         # 5 trees per client (total: 44×5 = 220 trees)

# ===== SHARED CONFIGURATION =====
fraction-train = 1.0        # Bagging: % of clients per round; Cyclic: ignored (always 1 client)
fraction-evaluate = 1.0     # % of clients for evaluation

# ===== DATA CONFIGURATION =====
keep-location = false       # false: drop Location (federated default)
                           # true: encode Location as feature

# ===== XGBOOST HYPERPARAMETERS =====
# Based on best hyperparameters from centralized training
# Note: These are automatically synchronized via sync_hyperparameters.ps1
params.objective = "binary:logistic"
params.eta = 0.0398532599330044
params.max-depth = 6
params.min-child-weight = 1
params.gamma = 0.399713685294389
params.subsample = 0.723611953058621
params.colsample-bytree = 0.878466831713619
params.eval-metric = "aucpr"
params.scale-pos-weight = 3.51387619917771
params.nthread = 16
params.tree-method = "hist"
params.reg-alpha = 0.000357124347353669
params.reg-lambda = 0.00344698976144497

# ===== FEDERATION CONFIGURATION =====
[tool.flwr.federations]
default = "local-simulation"

[tool.flwr.federations.local-simulation]
options.num-supernodes = 44  # 44 clients (one per Location)
